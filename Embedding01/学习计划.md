# Embedding 向量化学习计划

> 目标：使用亚马逊美食评论数据集，基于本地部署的开源 OpenAI 兼容接口，完成 Embedding 向量化的完整链路学习。结果保存为 CSV 文件，不依赖向量数据库。

---

## 技术选型

| 层次 | 技术方案 | 理由 |
|------|----------|------|
| Embedding 推理服务 | **Ollama** | 开源、支持 GPU 加速、内置 `/v1/embeddings` OpenAI 兼容接口，一键部署 |
| Embedding 模型 | `nomic-embed-text` | 高质量开源英文 Embedding 模型，768 维，Ollama 官方支持 |
| 向量存储 | **CSV 文件** | 简单直观，便于学习阶段查看和调试 |
| 相似度计算 | **numpy** | 手动实现余弦相似度，理解底层原理 |
| 数据集 | Amazon Fine Food Reviews | Kaggle 公开数据集，~568K 条英文食品评论 |
| 编程语言 | Python 3.10+ | 生态最完整 |
| 客户端 SDK | `openai` Python SDK | 直接对接 Ollama 的 OpenAI 兼容接口 |

---

## 整体架构

```
Amazon Fine Food Reviews (Reviews.csv)
        │
        ▼
  02_prepare_data.py（清洗 → reviews_clean.csv）
        │
        ▼
  Ollama Embedding 服务  ◄──── RTX 5070Ti GPU 加速
  (localhost:11434/v1)
        │
        ▼
  03_embed_and_store.py（生成向量 → reviews_with_embeddings.csv）
        │
        ▼
  04_semantic_search.py（读取 CSV → 余弦相似度计算 → 输出 Top-K 结果）
```

---

## 学习阶段划分

### 阶段一：环境搭建

**目标：** 让 Ollama 跑起来，验证接口可用

#### 1.1 安装 Ollama（支持 GPU）

```bash
# Windows：从官网下载安装包
# https://ollama.com/download

# 拉取 Embedding 模型
ollama pull nomic-embed-text

# 验证 OpenAI 兼容接口
curl http://localhost:11434/v1/embeddings \
  -H "Content-Type: application/json" \
  -d '{"model": "nomic-embed-text", "input": "hello world"}'
```

返回 JSON 中包含 `"embedding": [0.123, -0.456, ...]` 即表示成功。

#### 1.2 安装 Python 依赖

```bash
pip install openai pandas numpy tqdm matplotlib
```

---

### 阶段二：理解 Embedding 基本概念

**目标：** 动手验证核心概念，建立直觉

#### 2.1 知识点清单

- [ ] 什么是 Embedding？文字 → 高维向量的过程
- [ ] 为什么语义相似的句子向量距离近？
- [ ] 余弦相似度公式：`cos(a, b) = (a·b) / (|a| × |b|)`，值域 [-1, 1]
- [ ] nomic-embed-text 的向量维度：768 维
- [ ] Token 长度限制（该模型最大 8192 tokens）

#### 2.2 概念详解

**768维是什么意思？**

一段文字被转换成一个包含 768 个小数的列表，每个数字代表文本在某个语义维度上的强度：

```
"This coffee is great."  →  [0.0073, 0.0382, -0.1788, ..., 0.0211]
                                          共 768 个数字
```

不管输入文本是 10 个字还是 1000 个字，输出**永远是固定的 768 维**。
模型做的事情就是把任意长度的文本"压缩提炼"成固定格式的语义摘要。

不同模型的维度对比：

| 模型 | 维度 |
|------|------|
| `nomic-embed-text` | 768 |
| `mxbai-embed-large` | 1024 |
| OpenAI `text-embedding-3-small` | 1536 |
| OpenAI `text-embedding-3-large` | 3072 |

维度越高，能表达的语义细节越丰富，计算量也越大。768 对学习和一般应用完全够用。

---

**Token 长度限制是什么意思？**

Token 是模型处理文本的最小单位，不是字也不是词，大概介于两者之间：

```
"I love coffee!"  →  ["I", " love", " coffee", "!"]  →  4 个 token
```

英文大约每 0.75 个单词 = 1 个 token。`nomic-embed-text` 最大支持 **8192 tokens**，约等于 6000 个英文单词，普通评论完全用不满。

超出限制的内容会被**直接截断忽略**，所以在 `02_prepare_data.py` 里提前做了字符数截断（2000字符），避免超长文本浪费推理时间。这和模型的 token 限制是两回事：

- **字符截断（我们做的）**：提前裁掉过长评论，节省时间
- **Token 限制（模型的）**：超出后模型自动丢弃尾部内容

---

**向量里的小数为什么都是 0.00 左右？**

模型训练时会对输出做归一化，让整个 768 维向量的 L2 范数（长度）接近 1：

```
√(0.007² + 0.038² + (-0.179)² + ... )  ≈  1.0
```

768 个数的平方和为 1，平均每个数的绝对值约为：

```
1 / √768 ≈ 0.036
```

所以大部分值落在 **-0.2 到 0.2** 之间完全正常，不是数据有问题。

单独看每个数字没有人类可读的含义，模型在训练中自己学出来的抽象特征。
只有把两个向量放在一起做余弦相似度比较，才能体现语义关系。

类比：RGB 颜色 `(255, 128, 0)` 表示橙色，单独看 `255` `128` `0` 没意义，组合在一起才有含义。Embedding 是 768 维的"语义颜色"。

---

**np.stack 把向量列表变成矩阵**

CSV 里 embedding 列存的是 JSON 字符串，读取时需要两步还原：

```python
# 第一步：JSON 字符串 → list[float]
"[0.007, 0.038, -0.179]"  →  json.loads()  →  [0.007, 0.038, -0.179]

# 第二步：1000 个独立列表 → 一张二维矩阵
[0.007, 0.038, -0.179, ...]   ┐              ┌──────────────────────┐
[0.011, -0.02,  0.441, ...]   ├─ np.stack ─→ │ 0.007  0.038  -0.179 │ 第0行
[0.033,  0.15, -0.088, ...]   ┘              │ 0.011 -0.020   0.441 │ 第1行
...（共1000个）                               │ ...                  │
                                             └──────────────────────┘
                                               shape: (1000, 768)
```

变成矩阵的好处：后续搜索时用**一次矩阵乘法**同时算出查询向量与全部 1000 条的相似度，而不需要写循环逐条计算，速度快得多。

---

**为什么要用向量搜索，传统数据库不能做吗？**

核心区别：**传统数据库搜的是字面，向量搜的是语义。**

```sql
-- 传统关键词搜索
SELECT * FROM reviews WHERE text LIKE '%dog food%'
-- 只能找到包含"dog food"这两个词的评论
```

如果评论写的是：
- `"my puppy loves this kibble"` → 找不到（没有"dog food"）
- `"great for my golden retriever"` → 找不到
- `"canine nutrition product"` → 找不到

向量搜索查询 `"dog food my pet loves"` 时，上面三句的向量和查询向量距离很近，**全都能找到**，即使一个相同的词都没有。

| 查询 | 传统搜索 | 向量搜索 |
|------|----------|----------|
| `"terrible"` | 只找含"terrible"的 | 还能找到"awful""waste of money""disgusted" |
| `"好吃的咖啡"` | 找不到任何英文评论 | 能找到语义对应的英文评论 |
| `"产品坏了"` | 完全找不到 | 能匹配"arrived broken""damaged packaging" |

> 传统搜索要求你**知道对方用了什么词**，向量搜索要求你**表达你想找什么意思**。

---

**向量技术不是大模型专属的**

向量化这个思路比大模型早很多年，各领域早已在用：

| 时间 | 事件 |
|------|------|
| 2013年 | Word2Vec 出现，搜索引擎开始用向量表示词语 |
| 2016年左右 | 电商推荐系统大规模用向量做"猜你喜欢" |
| 2019年 | BERT 出现，句子级别的向量质量大幅提升 |
| 2022年后 | ChatGPT 爆发，RAG 架构流行，Embedding 才被更多开发者关注 |

各场景的实际应用：

| 场景 | 用法 |
|------|------|
| 搜索引擎 | 查询和文档都向量化，语义匹配而非关键词匹配 |
| 电商推荐 | 用户行为、商品描述向量化，找相似用户/相似商品 |
| 音乐/视频推荐 | Spotify、YouTube 把内容特征向量化做相似推荐 |
| 图片搜索 | 图片也能向量化，"以图搜图"就是找向量最近的图片 |
| 反欺诈 | 把用户行为序列向量化，异常行为向量会偏离正常区域 |
| RAG / 大模型 | 给 LLM 提供相关上下文，就是向量检索 |

大模型本身不擅长"记住大量文档"，Embedding + 向量检索可以弥补这一点。RAG 架构把两者绑在一起，才让向量技术在大模型时代被更多人关注，但向量本身是独立的技术。

#### 2.2 动手实验：`01_embedding_basics.py`

```
任务：
1. 用 openai SDK 调用 Ollama 接口，生成几个句子的 Embedding
2. 用 numpy 手动计算两个向量的余弦相似度
3. 对比：相似句子 vs 不相关句子 的相似度数值
4. 用 matplotlib 画出多个句子之间的相似度热力图
```

---

### 阶段三：数据集准备

**目标：** 下载并清洗亚马逊美食数据集

#### 3.1 数据集获取

- Kaggle 页面：`Amazon Fine Food Reviews`
- 下载文件：`Reviews.csv`（约 290 MB）
- 放入项目的 `data/` 目录

主要字段说明：

| 字段 | 说明 |
|------|------|
| `Id` | 评论 ID |
| `ProductId` | 商品 ID |
| `UserId` | 用户 ID |
| `Score` | 评分（1-5星） |
| `Summary` | 评论标题 |
| `Text` | 评论正文 |

#### 3.2 数据预处理脚本：`02_prepare_data.py`

```
任务：
1. 加载 Reviews.csv
2. 去除重复评论（ProductId + UserId 去重，保留 Score 最新的）
3. 过滤掉 Summary 或 Text 为空的行
4. 合并 Summary + ". " + Text 为单一 content 字段
5. 只保留列：Id, ProductId, Score, content
6. 取前 1,000 条作为学习子集（避免等待太久）
7. 保存为 data/reviews_clean.csv
```

---

### 阶段四：批量生成 Embedding 并保存

**目标：** 将 1,000 条评论向量化，结果追加写入 CSV

#### 4.1 核心脚本：`03_embed_and_store.py`

```
任务：
1. 读取 data/reviews_clean.csv
2. 初始化 OpenAI 客户端，指向 Ollama 本地接口
   base_url = "http://localhost:11434/v1"
   api_key  = "ollama"      # 任意字符串即可

3. 分批（batch_size=16）调用 embeddings.create()
   - 每批返回 16 个 768 维向量
   - 将向量序列化为字符串存入新列 embedding

4. 用 tqdm 显示进度条

5. 保存为 data/reviews_with_embeddings.csv
   列：Id, ProductId, Score, content, embedding
```

#### 4.2 关键代码片段

```python
from openai import OpenAI
import pandas as pd, json, numpy as np

client = OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama"
)

# 生成单条 Embedding
response = client.embeddings.create(
    model="nomic-embed-text",
    input=["This coffee tastes amazing!"]
)
vector = response.data[0].embedding   # list[float], 长度 768

# 序列化存入 CSV
row["embedding"] = json.dumps(vector)
```

> **注意：** CSV 中的 embedding 列存储 JSON 字符串，读取时用 `json.loads()` 还原为列表。

---

### 阶段五：语义搜索 Demo

**目标：** 纯 numpy 实现相似度检索，不依赖任何数据库

#### 5.1 搜索脚本：`04_semantic_search.py`

```
任务：
1. 读取 data/reviews_with_embeddings.csv
2. 将所有 embedding 列解析为 numpy 矩阵（shape: 1000 × 768）
3. 接收用户输入的查询语句
4. 调用 Ollama 生成查询向量（768 维）
5. 用矩阵运算一次性计算与所有行的余弦相似度
6. 取 Top-5，格式化输出：评分、相似度分数、评论摘要
```

#### 5.2 余弦相似度矩阵运算示意

```python
import numpy as np

# embeddings_matrix: shape (N, 768)
# query_vec: shape (768,)

# 归一化
norm_matrix = embeddings_matrix / np.linalg.norm(embeddings_matrix, axis=1, keepdims=True)
norm_query  = query_vec / np.linalg.norm(query_vec)

# 一次矩阵乘法得到所有相似度
scores = norm_matrix @ norm_query   # shape (N,)

top5_idx = np.argsort(scores)[::-1][:5]
```

#### 5.3 搜索示例查询

```
- "great chocolate taste but too sweet"
- "dog food my pet loves"
- "terrible product, waste of money"
- "best coffee I ever tried"
```

---

### 阶段六：进阶实验（可选）

**目标：** 加深理解

#### 6.1 降维可视化
用 UMAP 将 768 维向量降到 2D，按评分（1-5星）着色，观察聚类分布

```bash
pip install umap-learn
```

#### 6.2 对比不同 Embedding 模型
拉取 `mxbai-embed-large`（1024 维），分别生成向量，对比相同查询的检索结果差异

```bash
ollama pull mxbai-embed-large
```

#### 6.3 RAG 初体验（检索增强生成）
将 Top-5 检索结果拼入 prompt，调用 Ollama 的 chat 接口生成总结回答

---

## 文件结构规划

```
Embedding01/
├── 学习计划.md                      ← 本文件
├── data/
│   ├── Reviews.csv                  ← 原始数据集（手动从 Kaggle 下载）
│   ├── reviews_clean.csv            ← 清洗后的 1000 条子集
│   └── reviews_with_embeddings.csv  ← 含 embedding 列的最终文件
├── 01_embedding_basics.py           ← 概念验证：调接口、算相似度、画热力图
├── 02_prepare_data.py               ← 数据清洗
├── 03_embed_and_store.py            ← 批量向量化，写入 CSV
├── 04_semantic_search.py            ← 语义搜索 Demo
└── requirements.txt
```

---

## requirements.txt

```
openai>=1.0.0
pandas>=2.0.0
numpy>=1.26.0
tqdm>=4.66.0
matplotlib>=3.7.0
```

---

## 快速开始检查清单

- [ ] Ollama 已安装并运行（`ollama serve`）
- [ ] 已拉取 `nomic-embed-text` 模型（`ollama pull nomic-embed-text`）
- [ ] 已从 Kaggle 下载 `Reviews.csv` 并放入 `data/` 目录
- [ ] Python 依赖已安装（`pip install -r requirements.txt`）
- [ ] 运行 `01_embedding_basics.py` 验证接口连通性

---

## 参考资源

- [Ollama 官网](https://ollama.com) — 本地 Embedding 推理服务
- [nomic-embed-text 模型](https://ollama.com/library/nomic-embed-text) — Ollama 模型库
- [Amazon Fine Food Reviews - Kaggle](https://www.kaggle.com/datasets/snap/amazon-fine-food-reviews) — 数据集下载
- [OpenAI Python SDK](https://github.com/openai/openai-python) — 兼容 Ollama 接口
