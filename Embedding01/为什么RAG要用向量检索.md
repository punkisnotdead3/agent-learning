# 为什么 RAG 一定要用向量检索？

## 核心问题

获取到文档数据后，为什么不直接把所有内容交给 LLM 检索，而是要先转成向量，再让模型整理回答？

---

## 原因一：Context Window 的物理限制

大模型能"看到"的内容是有上限的，这个上限叫 **Context Window（上下文窗口）**。

```
假设你有一份 1000 页的公司手册，用户问"年假有几天"：

直接塞给模型：
  把 1000 页全部发给模型 → 模型读完再回答

问题：
  ❌ token 限制    —— 大多数模型 context window 有上限，1000 页根本放不进去
  ❌ 成本极高      —— 按 token 收费，每次问一个问题都要发 1000 页
  ❌ 速度很慢      —— 模型处理的内容越多，响应越慢
  ❌ 注意力稀释    —— 内容太多，模型反而找不到重点，回答质量下降
```

| 模型 | Context Window 大约上限 |
|------|------------------------|
| DeepSeek-V3 | 64K tokens ≈ 约 10 万字 |
| GPT-4o | 128K tokens ≈ 约 20 万字 |
| Claude 3.5 | 200K tokens ≈ 约 30 万字 |

企业内部文档动辄几百万字，任何模型都装不下。

---

## 原因二：向量检索解决了什么

```
向量检索的方式：

  【准备阶段（一次性）】
  把 1000 页切成 5000 个小块
      ↓
  每块向量化后存入向量数据库

  【查询阶段（每次问答）】
  用户问"年假有几天"
      ↓
  把问题也向量化
      ↓
  在向量数据库里找最相似的 3~5 个块（可能只有半页内容）
      ↓
  只把这 3~5 块 + 用户问题发给模型
      ↓
  模型基于这几段内容给出回答
```

效果对比：

| | 直接塞给模型 | 向量检索 + RAG |
|--|------------|--------------|
| 发送内容量 | 1000 页 | 3~5 个小块 |
| token 消耗 | 极高 | 极低（降低 99%）|
| 响应速度 | 很慢 | 快 |
| 回答准确率 | 低（注意力稀释）| 高（聚焦相关内容）|
| 能否超长文档 | ❌ | ✅ |

---

## 原因三：向量 = 语义，不是关键词

这是向量化最关键的价值：**语义相近的文字，向量数值就相近**。

```python
"年假"    → [0.2,  0.8,  0.1, ...]
"带薪假期" → [0.21, 0.79, 0.11, ...]  ← 数值很接近（语义相近）
"数据库"  → [0.9,  0.1,  0.6, ...]   ← 数值差很远（语义无关）
```

这意味着：

```
用户问："我能休息几天"

关键词匹配（传统搜索）：
  找"休息"→ 找不到（文档里写的是"年假天数"）❌

向量检索：
  "休息几天" 和 "年假天数" 语义相近 → 能找到 ✅
```

向量检索比关键词匹配更**智能**，能理解同义词、近义词、上下位关系。

---

## 整体架构图

```
外部文档（PDF / 网页 / 数据库 / Word）
          ↓
   Document Loaders（加载）
          ↓
   Text Splitters（切成小块）
          ↓
   Embeddings（每块转成向量）
          ↓
   Vector Store（存储向量）
          ↓
          ↑ ←←←←← 用户提问（也转成向量）
   相似度搜索（找最近的 Top-K 块）
          ↓
   把检索结果 + 用户问题 → 发给 LLM
          ↓
   LLM 基于检索内容生成回答
          ↓
        用户
```

---

## 模型在 RAG 中的真正角色

> 模型**不负责检索**，只负责最后一步：**把检索到的相关片段整理成自然语言回答**。

```
检索的事  → 向量数据库来做（快、准、便宜）
回答的事  → LLM 来做（理解语义、生成流畅回答）
```

两者各司其职，这就是 RAG 的核心设计思路。

---

## 一句话总结

> 向量检索不是为了"让模型更聪明"，而是为了**只给模型看相关的那一小段**，
> 突破 context window 限制、大幅降低成本、提高回答准确率。
